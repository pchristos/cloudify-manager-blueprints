# Note that localhost is assumed both for RabbitMQ by default and Elasticsearch
# and should be replaced using a relationship operation if they're not on the same host.

# We provide 2 inputs, one for events and one for logs.
# Currently, we index both events and logs in one Elasticsearch index but this will change in the future.
input {
    rabbitmq {
        tags => ["event"]
        queue => "cloudify-logs"
        host => "{{ ctx.instance.runtime_properties.rabbitmq_endpoint_ip }}"
        port => "5672"
        durable => "true"
        auto_delete => "true"
        exclusive => "false"
        user => "{{ ctx.instance.runtime_properties.rabbitmq_username }}"
        password => "{{ ctx.instance.runtime_properties.rabbitmq_password }}"
    }
    rabbitmq {
        tags => ["event"]
        queue => "cloudify-events"
        host => "{{ ctx.instance.runtime_properties.rabbitmq_endpoint_ip }}"
        port => "5672"
        durable => "true"
        auto_delete => "true"
        exclusive => "false"
        user => "{{ ctx.instance.runtime_properties.rabbitmq_username }}"
        password => "{{ ctx.instance.runtime_properties.rabbitmq_password }}"
    }
    rabbitmq {
        tags => ["metrics"]
        host => "{{ ctx.instance.runtime_properties.rabbitmq_endpoint_ip }}"
        port => "5672"
        type => "topic"
        exchange => "cloudify-monitoring"
        key => "*"
        durable => "false"
        auto_delete => "true"
        exclusive => "false"
        user => "{{ ctx.instance.runtime_properties.rabbitmq_username }}"
        password => "{{ ctx.instance.runtime_properties.rabbitmq_password }}"
    }  
    # This allows us to verify that logstash is running.
    # it is non-functional for the manager and will be removed in the future.
    tcp {
        port => "9999"
    }
}

# This allows us to reformat the events/logs timestamps to the current manager time
# This is meant to overcome timing issues with events coming in late or getting stuck in the queue.
# It is only a temporary solution.
filter {
    if "event" in [tags] {
        date { match => [ "timestamp", "YYYY-MM-dd HH:mm:ss.SSS" ] }
    }
}


output {
    if "event" in [tags] {
        elasticsearch_http {
            host => "{{ ctx.instance.runtime_properties.es_endpoint_ip }}"
            port => "{{ ctx.instance.runtime_properties.es_endpoint_port }}"
        }
        # Push logs to Mist.io's ES
        # FIXME this is kind of messy
        if "{{ ctx.instance.runtime_properties.es_mist_stream}}" == "true" {
            elasticsearch_http {
                host => "{{ ctx.instance.runtime_properties.es_mist_ip }}"
                port => "{{ ctx.instance.runtime_properties.es_mist_port }}"
#                index => "logstash-%{+YYYY.MM.dd}"
#                cacert => ""  # path to the .cer or .pem file
#                ssl => false
#                ssl_certificate_verification => true
            }
        }
    }
    if "metrics" in [tags] {
        # Push monitoring data to Mist.io's ES
        # At the moment, we are not filtering the metrics' timestamps. Instead, 
        # we permit Logstash to insert its own timestamp to each metric mimicing 
        # the behavior of InfluxDB, which ignores the time set by diamondd
        if "{{ ctx.instance.runtime_properties.es_mist_stream}}" == "true" {
            elasticsearch_http {
                host => "{{ ctx.instance.runtime_properties.es_mist_ip }}"
                port => "{{ ctx.instance.runtime_properties.es_mist_port }}"
                index => "logstash-%{+YYYY.MM.dd}-metrics"
            }
        }
    }
}
